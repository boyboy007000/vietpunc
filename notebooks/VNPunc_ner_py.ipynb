{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VNPunc - ner_py",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ojUkQE0Ef7XpdqYpuGoXXLsEzdIEZq8E",
      "authorship_tag": "ABX9TyOzEQ5EUkNEoyTQd7d63oPy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heraclex12/VN-Punc-Pretrained-LMs/blob/main/VNPunc_ner_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq10jdSxp5Fh",
        "outputId": "e2b78f7e-5187-4f87-dc6b-af5c3fd2f941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "!pip3 install transformers\n",
        "\n",
        "import os\n",
        "os.chdir('drive/My Drive/vncorenlp/journal_hero')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.16.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Th8cyw6qBtI"
      },
      "source": [
        "# config\n",
        "\n",
        "max_paragraph_leng = 128\n",
        "max_char_leng = 20\n",
        "batch_size = 64\n",
        "special_tokens = ['<NUM>', '<URL>', '<EMAIL>']\n",
        "punctuation_marks = ['PERIOD', 'COMMA', 'COLON', 'QMARK', 'EXCLAM', 'SEMICOLON', 'O', 'PAD']\n",
        "eos_marks = ['PERIOD', 'QMARK', 'EXCLAM']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OS6uticr0zy",
        "outputId": "c8558d9d-521c-45e2-b4bd-182cfe707a84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile bert.py\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import csv\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import (BertForTokenClassification, BertTokenizer, BertConfig,\n",
        "                          AdamW, get_linear_schedule_with_warmup, BertModel)\n",
        "from torch import nn\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PunctuationPredictionModel(BertForTokenClassification):\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,valid_ids=None,attention_mask_label=None):\n",
        "        sequence_output = self.bert(input_ids, token_type_ids, attention_mask,head_mask=None)[0]\n",
        "        batch_size,max_len,feat_dim = sequence_output.shape\n",
        "        valid_output = torch.zeros(batch_size,max_len,feat_dim,dtype=torch.float32,device='cuda')\n",
        "        for i in range(batch_size):\n",
        "            jj = -1\n",
        "            for j in range(max_len):\n",
        "                    if valid_ids[i][j].item() == 1:\n",
        "                        jj += 1\n",
        "                        valid_output[i][jj] = sequence_output[i][j]\n",
        "        sequence_output = self.dropout(valid_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=0)\n",
        "            # Only keep active parts of the loss\n",
        "            #attention_mask_label = None\n",
        "            if attention_mask_label is not None:\n",
        "                active_loss = attention_mask_label.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
        "                active_labels = labels.view(-1)[active_loss]\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            return loss\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id, valid_ids=None, label_mask=None):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "        self.valid_ids = valid_ids\n",
        "        self.label_mask = label_mask\n",
        "\n",
        "def readfile(filename, eos_marks=['PERIOD', 'QMARK', 'EXCLAM']):\n",
        "  df = pd.read_csv(filename, encoding='utf-8', sep=' ', names=['token', 'label'], keep_default_na=False)\n",
        "  idx = 0\n",
        "  n_tokens = len(df)\n",
        "  paragraphs = []\n",
        "  token_labels = []\n",
        "  while idx < n_tokens and idx >= 0:\n",
        "    sub_df = df.iloc[idx: min(idx+128, n_tokens)]\n",
        "    end_idx = sub_df[sub_df.label.isin(eos_marks)].tail(1).index\n",
        "    if end_idx.empty:\n",
        "      end_idx = -1\n",
        "      paragraph_df = df.iloc[idx:]\n",
        "    else:\n",
        "      end_idx = end_idx.item() + 1\n",
        "      paragraph_df = df.iloc[idx: end_idx]\n",
        "\n",
        "    # numeric_labels = paragraph_df.label.apply(lambda l: punctuation_marks.index(l))\n",
        "    paragraphs.append(paragraph_df.token.values.tolist())\n",
        "    token_labels.append(paragraph_df.label.values.tolist())\n",
        "    idx = end_idx\n",
        "  return list(zip(paragraphs, token_labels))\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        return readfile(input_file)\n",
        "\n",
        "\n",
        "class PuncProcessor(DataProcessor):\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"train.txt\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"valid.txt\")), \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"test.txt\")), \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        return ['O', 'PERIOD', 'COMMA', 'COLON', 'QMARK', 'EXCLAM', 'SEMICOLON', '[CLS]', '[SEP]']\n",
        "\n",
        "    def _create_examples(self,lines,set_type):\n",
        "        examples = []\n",
        "        for i,(sentence,label) in enumerate(lines):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = ' '.join(sentence)\n",
        "            text_b = None\n",
        "            label = label\n",
        "            examples.append(InputExample(guid=guid,text_a=text_a,text_b=text_b,label=label))\n",
        "        return examples\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list,1)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index,example) in enumerate(examples):\n",
        "        textlist = example.text_a.split(' ')\n",
        "        labellist = example.label\n",
        "        tokens = []\n",
        "        labels = []\n",
        "        valid = []\n",
        "        label_mask = []\n",
        "        for i, word in enumerate(textlist):\n",
        "            if re.search(r'([+-]?\\d+[\\.,]?)+', word) is not None or word.isnumeric():\n",
        "              word = '<NUM>'\n",
        "            token = tokenizer.tokenize(word)\n",
        "            tokens.extend(token)\n",
        "            label_1 = labellist[i]\n",
        "            for m in range(len(token)):\n",
        "                if m == 0:\n",
        "                    labels.append(label_1)\n",
        "                    valid.append(1)\n",
        "                    label_mask.append(1)\n",
        "                else:\n",
        "                    valid.append(0)\n",
        "        if len(tokens) >= max_seq_length - 1:\n",
        "            tokens = tokens[0:(max_seq_length - 2)]\n",
        "            labels = labels[0:(max_seq_length - 2)]\n",
        "            valid = valid[0:(max_seq_length - 2)]\n",
        "            label_mask = label_mask[0:(max_seq_length - 2)]\n",
        "        ntokens = []\n",
        "        segment_ids = []\n",
        "        label_ids = []\n",
        "        ntokens.append(\"[CLS]\")\n",
        "        segment_ids.append(0)\n",
        "        valid.insert(0,1)\n",
        "        label_mask.insert(0,1)\n",
        "        label_ids.append(label_map[\"[CLS]\"])\n",
        "        for i, token in enumerate(tokens):\n",
        "            ntokens.append(token)\n",
        "            segment_ids.append(0)\n",
        "            if len(labels) > i:\n",
        "                label_ids.append(label_map[labels[i]])\n",
        "        ntokens.append(\"[SEP]\")\n",
        "        segment_ids.append(0)\n",
        "        valid.append(1)\n",
        "        label_mask.append(1)\n",
        "        label_ids.append(label_map[\"[SEP]\"])\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(ntokens)\n",
        "        input_mask = [1] * len(input_ids)\n",
        "        label_mask = [1] * len(label_ids)\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "            segment_ids.append(0)\n",
        "            label_ids.append(0)\n",
        "            valid.append(1)\n",
        "            label_mask.append(0)\n",
        "        while len(label_ids) < max_seq_length:\n",
        "            label_ids.append(0)\n",
        "            label_mask.append(0)\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "        assert len(label_ids) == max_seq_length\n",
        "        assert len(valid) == max_seq_length\n",
        "        assert len(label_mask) == max_seq_length\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                    [str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\n",
        "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            # logger.info(\"label: %s (id = %d)\" % (example.label, label_ids))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_id=label_ids,\n",
        "                              valid_ids=valid,\n",
        "                              label_mask=label_mask))\n",
        "    return features\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    ## Required parameters\n",
        "    parser.add_argument(\"--data_dir\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
        "    parser.add_argument(\"--bert_model\", default=None, type=str, required=True,\n",
        "                        help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
        "                        \"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"\n",
        "                        \"bert-base-multilingual-cased, bert-base-chinese.\")\n",
        "    parser.add_argument(\"--task_name\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"The name of the task to train.\")\n",
        "    parser.add_argument(\"--output_dir\",\n",
        "                        default=None,\n",
        "                        type=str,\n",
        "                        required=True,\n",
        "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
        "\n",
        "    ## Other parameters\n",
        "    parser.add_argument(\"--cache_dir\",\n",
        "                        default=\"\",\n",
        "                        type=str,\n",
        "                        help=\"Where do you want to store the pre-trained models downloaded from s3\")\n",
        "    parser.add_argument(\"--max_seq_length\",\n",
        "                        default=128,\n",
        "                        type=int,\n",
        "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
        "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
        "                             \"than this will be padded.\")\n",
        "    parser.add_argument(\"--do_train\",\n",
        "                        action='store_true',\n",
        "                        help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\",\n",
        "                        action='store_true',\n",
        "                        help=\"Whether to run eval or not.\")\n",
        "    parser.add_argument(\"--eval_on\",\n",
        "                        default=\"dev\",\n",
        "                        help=\"Whether to run eval on the dev set or test set.\")\n",
        "    parser.add_argument(\"--do_lower_case\",\n",
        "                        action='store_true',\n",
        "                        help=\"Set this flag if you are using an uncased model.\")\n",
        "    parser.add_argument(\"--train_batch_size\",\n",
        "                        default=32,\n",
        "                        type=int,\n",
        "                        help=\"Total batch size for training.\")\n",
        "    parser.add_argument(\"--eval_batch_size\",\n",
        "                        default=8,\n",
        "                        type=int,\n",
        "                        help=\"Total batch size for eval.\")\n",
        "    parser.add_argument(\"--learning_rate\",\n",
        "                        default=3e-5,\n",
        "                        type=float,\n",
        "                        help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--num_train_epochs\",\n",
        "                        default=3.0,\n",
        "                        type=float,\n",
        "                        help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\"--warmup_proportion\",\n",
        "                        default=0.1,\n",
        "                        type=float,\n",
        "                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
        "                             \"E.g., 0.1 = 10%% of training.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.01, type=float,\n",
        "                        help=\"Weight deay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
        "                        help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "                        help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--no_cuda\",\n",
        "                        action='store_true',\n",
        "                        help=\"Whether not to use CUDA when available\")\n",
        "    parser.add_argument(\"--local_rank\",\n",
        "                        type=int,\n",
        "                        default=-1,\n",
        "                        help=\"local_rank for distributed training on gpus\")\n",
        "    parser.add_argument('--seed',\n",
        "                        type=int,\n",
        "                        default=42,\n",
        "                        help=\"random seed for initialization\")\n",
        "    parser.add_argument('--gradient_accumulation_steps',\n",
        "                        type=int,\n",
        "                        default=1,\n",
        "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "    parser.add_argument('--fp16',\n",
        "                        action='store_true',\n",
        "                        help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
        "    parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
        "                        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "    parser.add_argument('--loss_scale',\n",
        "                        type=float, default=0,\n",
        "                        help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
        "                             \"0 (default value): dynamic loss scaling.\\n\"\n",
        "                             \"Positive power of 2: static loss scaling value.\\n\")\n",
        "    parser.add_argument('--server_ip', type=str, default='', help=\"Can be used for distant debugging.\")\n",
        "    parser.add_argument('--server_port', type=str, default='', help=\"Can be used for distant debugging.\")\n",
        "    args = parser.parse_args()\n",
        "    special_tokens = ['<NUM>', '<URL>', '<EMAIL>']\n",
        "\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    processors = {\"punctuation_prediction\":PuncProcessor}\n",
        "\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        n_gpu = torch.cuda.device_count()\n",
        "    else:\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        n_gpu = 1\n",
        "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
        "        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
        "\n",
        "    if args.gradient_accumulation_steps < 1:\n",
        "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
        "                            args.gradient_accumulation_steps))\n",
        "\n",
        "    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    if not args.do_train and not args.do_eval:\n",
        "        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
        "\n",
        "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n",
        "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    task_name = args.task_name.lower()\n",
        "\n",
        "    if task_name not in processors:\n",
        "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
        "\n",
        "    processor = processors[task_name]()\n",
        "    label_list = processor.get_labels()\n",
        "    num_labels = len(label_list) + 1\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
        "    tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "\n",
        "    train_examples = None\n",
        "    num_train_optimization_steps = 0\n",
        "    if args.do_train:\n",
        "        train_examples = processor.get_train_examples(args.data_dir)\n",
        "        num_train_optimization_steps = int(\n",
        "            len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n",
        "        if args.local_rank != -1:\n",
        "            num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n",
        "\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
        "\n",
        "    # Prepare model\n",
        "    config = BertConfig.from_pretrained(args.bert_model, num_labels=num_labels, finetuning_task=args.task_name)\n",
        "    model = PunctuationPredictionModel.from_pretrained(args.bert_model,\n",
        "              from_tf = False,\n",
        "              config = config)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias','LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    warmup_steps = int(args.warmup_proportion * num_train_optimization_steps)\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_train_optimization_steps)\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank,\n",
        "                                                          find_unused_parameters=True)\n",
        "\n",
        "    global_step = 0\n",
        "    nb_tr_steps = 0\n",
        "    tr_loss = 0\n",
        "    label_map = {i : label for i, label in enumerate(label_list,1)}\n",
        "    if args.do_train:\n",
        "        train_features = convert_examples_to_features(\n",
        "            train_examples, label_list, args.max_seq_length, tokenizer)\n",
        "        logger.info(\"***** Running training *****\")\n",
        "        logger.info(\"  Num examples = %d\", len(train_examples))\n",
        "        logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
        "        logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
        "        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
        "        all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)\n",
        "        all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)\n",
        "        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids,all_valid_ids,all_lmask_ids)\n",
        "        if args.local_rank == -1:\n",
        "            train_sampler = RandomSampler(train_data)\n",
        "        else:\n",
        "            train_sampler = DistributedSampler(train_data)\n",
        "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "        model.train()\n",
        "        for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
        "            tr_loss = 0\n",
        "            nb_tr_examples, nb_tr_steps = 0, 0\n",
        "            for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                input_ids, input_mask, segment_ids, label_ids, valid_ids,l_mask = batch\n",
        "                loss = model(input_ids, segment_ids, input_mask, label_ids,valid_ids,l_mask)\n",
        "                if n_gpu > 1:\n",
        "                    loss = loss.mean() # mean() to average on multi-gpu.\n",
        "                if args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "                if args.fp16:\n",
        "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                        scaled_loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "                tr_loss += loss.item()\n",
        "                nb_tr_examples += input_ids.size(0)\n",
        "                nb_tr_steps += 1\n",
        "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()  # Update learning rate schedule\n",
        "                    model.zero_grad()\n",
        "                    global_step += 1\n",
        "\n",
        "        # Save a trained model and the associated configuration\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "        model_to_save.save_pretrained(args.output_dir)\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "        label_map = {i : label for i, label in enumerate(label_list,1)}\n",
        "        model_config = {\"bert_model\":args.bert_model,\"do_lower\":args.do_lower_case,\"max_seq_length\":args.max_seq_length,\"num_labels\":len(label_list)+1,\"label_map\":label_map}\n",
        "        json.dump(model_config,open(os.path.join(args.output_dir,\"model_config.json\"),\"w\"))\n",
        "        # Load a trained model and config that you have fine-tuned\n",
        "    else:\n",
        "        # Load a trained model and vocabulary that you have fine-tuned\n",
        "        model = PunctuationPredictionModel.from_pretrained(args.output_dir)\n",
        "        tokenizer = BertTokenizer.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    if args.do_eval and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "        if args.eval_on == \"dev\":\n",
        "            eval_examples = processor.get_dev_examples(args.data_dir)\n",
        "        elif args.eval_on == \"test\":\n",
        "            eval_examples = processor.get_test_examples(args.data_dir)\n",
        "        else:\n",
        "            raise ValueError(\"eval on dev or test set only\")\n",
        "        eval_features = convert_examples_to_features(eval_examples, label_list, args.max_seq_length, tokenizer)\n",
        "        logger.info(\"***** Running evaluation *****\")\n",
        "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
        "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
        "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
        "        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
        "        all_valid_ids = torch.tensor([f.valid_ids for f in eval_features], dtype=torch.long)\n",
        "        all_lmask_ids = torch.tensor([f.label_mask for f in eval_features], dtype=torch.long)\n",
        "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids,all_valid_ids,all_lmask_ids)\n",
        "        # Run prediction for full data\n",
        "        eval_sampler = SequentialSampler(eval_data)\n",
        "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        label_map = {i : label for i, label in enumerate(label_list,1)}\n",
        "        for input_ids, input_mask, segment_ids, label_ids,valid_ids,l_mask in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = input_ids.to(device)\n",
        "            input_mask = input_mask.to(device)\n",
        "            segment_ids = segment_ids.to(device)\n",
        "            valid_ids = valid_ids.to(device)\n",
        "            label_ids = label_ids.to(device)\n",
        "            l_mask = l_mask.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(input_ids, segment_ids, input_mask,valid_ids=valid_ids,attention_mask_label=l_mask)\n",
        "\n",
        "            logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = label_ids.to('cpu').numpy()\n",
        "            input_mask = input_mask.to('cpu').numpy()\n",
        "\n",
        "            for i, label in enumerate(label_ids):\n",
        "                temp_1 = []\n",
        "                temp_2 = []\n",
        "                for j,m in enumerate(label):\n",
        "                    if j == 0:\n",
        "                        continue\n",
        "                    elif label_ids[i][j] == len(label_map):\n",
        "                        y_true.extend(temp_1)\n",
        "                        y_pred.extend(temp_2)\n",
        "                        break\n",
        "                    else:\n",
        "                        temp_1.append(label_map[label_ids[i][j]])\n",
        "                        temp_2.append(label_map[logits[i][j]])\n",
        "\n",
        "        punc_marks = ['PERIOD', 'COMMA', 'COLON', 'QMARK', 'EXCLAM', 'SEMICOLON']\n",
        "        report = classification_report(y_true, y_pred,digits=4, labels=punc_marks)\n",
        "        logger.info(\"\\n%s\", report)\n",
        "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
        "\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            logger.info(\"***** Eval results *****\")\n",
        "            logger.info(\"\\n%s\", report)\n",
        "            writer.write(report)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting bert.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWDOguLTr01_",
        "outputId": "68c4b06a-0dfd-4a68-cb29-e970fb4e9eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python bert.py --data_dir=punctuation/Novels/ --bert_model=bert-base-multilingual-uncased --task_name=punctuation_prediction --output_dir=out_ner --train_batch_size=64 --max_seq_length=190 --do_train --do_lower_case --num_train_epochs=5 --do_eval --eval_on='test' --warmup_proportion=0.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-26 08:26:20.803582: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "10/26/2020 08:26:22 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing PunctuationPredictionModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing PunctuationPredictionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing PunctuationPredictionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of PunctuationPredictionModel were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   *** Example ***\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   guid: train-0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   tokens: thuc sam soi to giay tren tay no phan van lat toi lat lui chua biet tinh the nao chieu hom qua luc tho tay vao ngan ban no tinh co phat hien ra to giay nay trong đo chi ven ven co may dong đuoc viet bang loi chu in hoa đoc thoa ##ng qua thuc bat giac đo bung mat cho minh lam quen voi minh o lop buoi sang ngoi cung cho voi ban đay neu khong no tu choi ban viet cho minh vai chu thanh that cam ta rat mong hoi am la thu chi ngan ngu ##i co vay ben duoi ky ten phong khe thuc von nhu ##t nhat lai ca then\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   input_ids: 101 11663 12603 34503 10114 58652 12424 13918 10181 12097 10147 12615 13685 12615 10921 15741 15680 11780 10103 11373 22913 23016 12626 13002 15151 13918 11614 10703 10972 10181 11780 10348 13441 12629 11742 10114 58652 10804 10492 11042 11581 22712 22712 10348 10431 19444 10554 13182 12221 13701 12895 10104 11294 19585 95154 10422 12626 11663 11226 48280 11042 43716 12152 11268 13718 12370 41243 10940 13718 157 33353 56978 12673 24282 11427 11268 10940 10972 15102 13468 11808 10181 10689 19176 10972 13182 11268 13718 13690 12895 11333 10203 17009 10546 14543 33128 13352 10345 10106 11778 11581 10703 20302 10116 10348 22783 10771 21293 12101 11201 14233 80208 11663 10168 12587 10123 12503 12295 10678 11120 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   *** Example ***\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   guid: train-1\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   tokens: no chua gap phai kieu lam quen đuong đot nhu the nay bao gio tu hom qua đen gio la thu cua anh chang phong khe chua ro mat mui no cu bat thuc nghi ngoi hoai đa may lan thuc đinh hoi y kien xuyen va cuc huong nhung lai so tui no choc thuc đanh giau nhe ##m ai chu con xuyen va cuc huong đa mo mien ##g choc ghe ##o thuc chi co nuoc bit tai quay mat đi cho khac lo ##ay hoa ##y mot hoi khong biet xu su the nao thuc khe buong tieng tho dai no khong biet nen pho ##t lo hay nen tra loi va neu tra loi thi phai viet nhung gi\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   input_ids: 101 10181 15741 19775 14692 37100 12370 41243 15703 36308 12587 10103 10804 12658 22935 10689 23016 12626 11928 22935 10106 11778 10500 12466 20959 14233 80208 15741 17675 12152 45447 10181 10707 11226 11663 20266 24282 84529 11135 10431 11267 11663 12969 13352 167 18272 45916 10222 20380 17282 11439 12295 10297 86836 10181 35619 11663 18039 87544 83747 10150 11279 12895 10173 45916 10222 20380 17282 11135 10837 19429 10251 35619 65104 10132 11663 11581 10348 13152 16464 10994 32264 12152 17170 11268 14240 10387 14519 11294 10158 10381 13352 11808 15680 22286 10192 10103 11373 11663 80208 80531 14648 15151 12533 10181 11808 15680 13791 14061 10123 10387 13055 13791 11234 13701 10222 13468 11234 13701 12059 14692 13182 11439 21464 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   *** Example ***\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   guid: train-2\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   tokens: nam ngoai cuc huong cung roi vo mot tinh huong tuong tu tham chi con ghe gom hon hung quan nhe ##t thu tong tinh vao ngan ban lai con đe doa neu khong tra loi cuc huong se an han nghe phat khi ##ep nhung cuc huong ban linh hon thuc nhieu no thang tay nem nhung toi hau thu cua hung quan vao sot rac luc cao hung no con gio thu ra cho xuyen va thuc cung đoc ca ba vua phan tich vua cuoi nga ##t nghe ##o thuc khong dam hanh đong nhu cuc huong cai cau neu khong no tu choi khien thuc thay toi toi hon nua anh chang phong khe khac xa hung quan anh ta khong he doa dam thuc\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   input_ids: 101 10423 16797 20380 17282 11427 13277 11821 10381 11780 17282 13511 10689 15571 11581 10173 65104 16444 11817 17522 10910 83747 10123 11778 14062 11780 11614 10703 10972 12295 10173 11654 35679 13468 11808 11234 13701 20380 17282 10128 10144 10210 19246 13441 11839 17768 11439 20380 17282 10972 20976 11817 11663 13699 10181 11452 13918 11550 11439 13685 14164 11778 10500 17522 10910 11614 30267 86422 13002 13326 17522 10181 10173 22935 11778 11742 11268 45916 10222 11663 11427 19585 10678 12314 20040 12097 15352 20040 19538 10122 10123 19246 10132 11663 11808 12235 13881 11326 12587 20380 17282 15685 14154 13468 11808 10181 10689 19176 29500 11663 14933 13685 13685 11817 22537 12466 20959 14233 80208 14240 12033 17522 10910 12466 10546 11808 10191 35679 12235 11663 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   *** Example ***\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   guid: train-3\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   tokens: lam gi ngoi than tho vay co nuong thuc lung tung đap vua noi no vua hap tap vo tron to giay lai giau trong long ban tay may đang on bai ha nghe cuc huong hoi vay thuc mung nhu bat đuoc vang u tao đang on lai bai hoa đot nhien cuc huong đoi giong thoi đi đung co doc may khong qua mat noi tao đau hay khai that đi thai đo cua cuc huong khien thuc đam cho ##t da tao đang on lai bai hoa that ma hu hoa hoa phep thi co cuc huong hat mai toc may on bai sao tao khong thay may gio tap tap ha thuc lie ##m moi tao cat vao roi\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   input_ids: 101 12370 21464 24282 10948 15151 22783 10348 93621 11663 29054 19921 46887 20040 12132 10181 20040 55558 15050 11821 24559 10114 58652 12295 87544 10492 11134 10972 13918 10431 14144 10125 15038 10240 19246 20380 17282 13352 22783 11663 81302 12587 11226 10554 27403 163 13738 14144 10125 12295 15038 11294 36308 18008 20380 17282 11705 23084 13105 17170 26749 10348 22747 10431 11808 12626 12152 12132 13738 11104 13055 27380 10203 17170 14224 11042 10500 20380 17282 29500 11663 25247 11268 10123 10141 13738 14144 10125 12295 15038 11294 10203 10507 18368 11294 11294 33620 12059 10348 20380 17282 11193 10597 19870 10431 10125 15038 11132 13738 11808 14933 10431 22935 15050 15050 10240 11663 23249 10150 12277 13738 15048 11614 13277 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   *** Example ***\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   guid: train-4\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   tokens: may cat trong long ban tay may chu gi giong cuc huong bong tro nen tinh quai thuc đien ##g hon sao may biet tao đung sau lung may nay gio sao khong biet cuc huong vua cuoi hi hi vua chia tay ra đua tao coi thu cai gi vay thu tinh phai khong biet khong the giau đuoc thuc đanh ben len chia to giay nhau nat trong tay ra troi oi tinh du chua cho minh lam quen voi mui con hon vu linh ca vong co may đung co noi bay thuc đo mat cu nu tao noi co sach mach co chung đang hoang ma may dam ke ##u tao noi bay ha đung luc đo xuyen om cap buoc vao\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   input_ids: 101 10431 15048 10492 11134 10972 13918 10431 12895 21464 23084 20380 17282 17882 13210 13791 11780 48926 11663 17358 10251 11817 11132 10431 15680 13738 26749 11356 29054 10431 10804 22935 11132 11808 15680 20380 17282 20040 19538 11463 11463 20040 27883 13918 11742 21610 13738 21085 11778 15685 21464 22783 11778 11780 14692 11808 15680 11808 10103 87544 10554 11663 18039 10771 14537 27883 10114 58652 21136 18081 10492 13918 11742 35745 47647 11780 10169 15741 11268 13718 12370 41243 10940 45447 10173 11817 12310 20976 10678 21780 10348 10431 26749 10348 12132 11836 11663 11042 12152 10707 10993 13738 12132 10348 21843 26644 10348 12874 14144 15916 10507 10431 12235 11009 10136 13738 12132 11836 10240 26749 13002 11042 45916 10230 11802 32615 11614 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:26:44 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "10/26/2020 08:28:37 - INFO - __main__ -   ***** Running training *****\n",
            "10/26/2020 08:28:37 - INFO - __main__ -     Num examples = 11447\n",
            "10/26/2020 08:28:37 - INFO - __main__ -     Batch size = 64\n",
            "10/26/2020 08:28:37 - INFO - __main__ -     Num steps = 890\n",
            "Epoch:   0% 0/5 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/179 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/179 [00:04<14:14,  4.80s/it]\u001b[A\n",
            "Iteration:   1% 2/179 [00:09<14:08,  4.80s/it]\u001b[A\n",
            "Iteration:   2% 3/179 [00:14<14:05,  4.80s/it]\u001b[A\n",
            "Iteration:   2% 4/179 [00:19<14:00,  4.80s/it]\u001b[A\n",
            "Iteration:   3% 5/179 [00:24<13:56,  4.81s/it]\u001b[A\n",
            "Iteration:   3% 6/179 [00:28<13:54,  4.82s/it]\u001b[A\n",
            "Iteration:   4% 7/179 [00:33<13:48,  4.82s/it]\u001b[A\n",
            "Iteration:   4% 8/179 [00:38<13:43,  4.81s/it]\u001b[A\n",
            "Iteration:   5% 9/179 [00:43<13:38,  4.82s/it]\u001b[A\n",
            "Iteration:   6% 10/179 [00:48<13:35,  4.82s/it]\u001b[A\n",
            "Iteration:   6% 11/179 [00:52<13:30,  4.83s/it]\u001b[A\n",
            "Iteration:   7% 12/179 [00:57<13:25,  4.82s/it]\u001b[A\n",
            "Iteration:   7% 13/179 [01:02<13:19,  4.81s/it]\u001b[A\n",
            "Iteration:   8% 14/179 [01:07<13:15,  4.82s/it]\u001b[A\n",
            "Iteration:   8% 15/179 [01:12<13:10,  4.82s/it]\u001b[A\n",
            "Iteration:   9% 16/179 [01:17<13:06,  4.83s/it]\u001b[A\n",
            "Iteration:   9% 17/179 [01:21<13:01,  4.82s/it]\u001b[A\n",
            "Iteration:  10% 18/179 [01:26<12:56,  4.82s/it]\u001b[A\n",
            "Iteration:  11% 19/179 [01:31<12:50,  4.82s/it]\u001b[A\n",
            "Iteration:  11% 20/179 [01:36<12:47,  4.82s/it]\u001b[A\n",
            "Iteration:  12% 21/179 [01:41<12:42,  4.83s/it]\u001b[A\n",
            "Iteration:  12% 22/179 [01:46<12:38,  4.83s/it]\u001b[A\n",
            "Iteration:  13% 23/179 [01:50<12:33,  4.83s/it]\u001b[A\n",
            "Iteration:  13% 24/179 [01:55<12:27,  4.82s/it]\u001b[A\n",
            "Iteration:  14% 25/179 [02:00<12:22,  4.82s/it]\u001b[A\n",
            "Iteration:  15% 26/179 [02:05<12:18,  4.82s/it]\u001b[A\n",
            "Iteration:  15% 27/179 [02:10<12:13,  4.83s/it]\u001b[A\n",
            "Iteration:  16% 28/179 [02:15<12:08,  4.83s/it]\u001b[A\n",
            "Iteration:  16% 29/179 [02:19<12:03,  4.83s/it]\u001b[A\n",
            "Iteration:  17% 30/179 [02:24<11:59,  4.83s/it]\u001b[A\n",
            "Iteration:  17% 31/179 [02:29<11:53,  4.82s/it]\u001b[A\n",
            "Iteration:  18% 32/179 [02:34<11:50,  4.83s/it]\u001b[A\n",
            "Iteration:  18% 33/179 [02:39<11:45,  4.83s/it]\u001b[A\n",
            "Iteration:  19% 34/179 [02:43<11:40,  4.83s/it]\u001b[A\n",
            "Iteration:  20% 35/179 [02:48<11:35,  4.83s/it]\u001b[A\n",
            "Iteration:  20% 36/179 [02:53<11:30,  4.83s/it]\u001b[A\n",
            "Iteration:  21% 37/179 [02:58<11:25,  4.83s/it]\u001b[A\n",
            "Iteration:  21% 38/179 [03:03<11:20,  4.82s/it]\u001b[A\n",
            "Iteration:  22% 39/179 [03:08<11:15,  4.82s/it]\u001b[A\n",
            "Iteration:  22% 40/179 [03:12<11:09,  4.82s/it]\u001b[A\n",
            "Iteration:  23% 41/179 [03:17<11:04,  4.81s/it]\u001b[A\n",
            "Iteration:  23% 42/179 [03:22<10:58,  4.81s/it]\u001b[A\n",
            "Iteration:  24% 43/179 [03:27<10:54,  4.81s/it]\u001b[A\n",
            "Iteration:  25% 44/179 [03:32<10:49,  4.81s/it]\u001b[A\n",
            "Iteration:  25% 45/179 [03:36<10:45,  4.81s/it]\u001b[A\n",
            "Iteration:  26% 46/179 [03:41<10:40,  4.81s/it]\u001b[A\n",
            "Iteration:  26% 47/179 [03:46<10:35,  4.82s/it]\u001b[A\n",
            "Iteration:  27% 48/179 [03:51<10:31,  4.82s/it]\u001b[A\n",
            "Iteration:  27% 49/179 [03:56<10:26,  4.82s/it]\u001b[A\n",
            "Iteration:  28% 50/179 [04:01<10:21,  4.82s/it]\u001b[A\n",
            "Iteration:  28% 51/179 [04:05<10:16,  4.81s/it]\u001b[A\n",
            "Iteration:  29% 52/179 [04:10<10:10,  4.81s/it]\u001b[A\n",
            "Iteration:  30% 53/179 [04:15<10:06,  4.81s/it]\u001b[A\n",
            "Iteration:  30% 54/179 [04:20<10:01,  4.82s/it]\u001b[A\n",
            "Iteration:  31% 55/179 [04:25<09:57,  4.82s/it]\u001b[A\n",
            "Iteration:  31% 56/179 [04:29<09:52,  4.82s/it]\u001b[A\n",
            "Iteration:  32% 57/179 [04:34<09:47,  4.81s/it]\u001b[A\n",
            "Iteration:  32% 58/179 [04:39<09:41,  4.81s/it]\u001b[A\n",
            "Iteration:  33% 59/179 [04:44<09:37,  4.81s/it]\u001b[A\n",
            "Iteration:  34% 60/179 [04:49<09:32,  4.81s/it]\u001b[A\n",
            "Iteration:  34% 61/179 [04:54<09:28,  4.82s/it]\u001b[A\n",
            "Iteration:  35% 62/179 [04:58<09:23,  4.81s/it]\u001b[A\n",
            "Iteration:  35% 63/179 [05:03<09:18,  4.81s/it]\u001b[A\n",
            "Iteration:  36% 64/179 [05:08<09:13,  4.81s/it]\u001b[A\n",
            "Iteration:  36% 65/179 [05:13<09:07,  4.80s/it]\u001b[A\n",
            "Iteration:  37% 66/179 [05:18<09:03,  4.81s/it]\u001b[A\n",
            "Iteration:  37% 67/179 [05:22<08:58,  4.81s/it]\u001b[A\n",
            "Iteration:  38% 68/179 [05:27<08:53,  4.81s/it]\u001b[A\n",
            "Iteration:  39% 69/179 [05:32<08:48,  4.80s/it]\u001b[A\n",
            "Iteration:  39% 70/179 [05:37<08:44,  4.81s/it]\u001b[A\n",
            "Iteration:  40% 71/179 [05:42<08:39,  4.81s/it]\u001b[A\n",
            "Iteration:  40% 72/179 [05:46<08:34,  4.81s/it]\u001b[A\n",
            "Iteration:  41% 73/179 [05:51<08:29,  4.81s/it]\u001b[A\n",
            "Iteration:  41% 74/179 [05:56<08:25,  4.81s/it]\u001b[A\n",
            "Iteration:  42% 75/179 [06:01<08:20,  4.81s/it]\u001b[A\n",
            "Iteration:  42% 76/179 [06:06<08:14,  4.80s/it]\u001b[A\n",
            "Iteration:  43% 77/179 [06:10<08:09,  4.80s/it]\u001b[A\n",
            "Iteration:  44% 78/179 [06:15<08:05,  4.80s/it]\u001b[A\n",
            "Iteration:  44% 79/179 [06:20<08:00,  4.81s/it]\u001b[A\n",
            "Iteration:  45% 80/179 [06:25<07:56,  4.81s/it]\u001b[A\n",
            "Iteration:  45% 81/179 [06:30<07:51,  4.81s/it]\u001b[A\n",
            "Iteration:  46% 82/179 [06:34<07:46,  4.81s/it]\u001b[A\n",
            "Iteration:  46% 83/179 [06:39<07:40,  4.80s/it]\u001b[A\n",
            "Iteration:  47% 84/179 [06:44<07:36,  4.80s/it]\u001b[A\n",
            "Iteration:  47% 85/179 [06:49<07:31,  4.81s/it]\u001b[A\n",
            "Iteration:  48% 86/179 [06:54<07:27,  4.81s/it]\u001b[A\n",
            "Iteration:  49% 87/179 [06:58<07:21,  4.80s/it]\u001b[A\n",
            "Iteration:  49% 88/179 [07:04<07:23,  4.88s/it]\u001b[A\n",
            "Iteration:  50% 89/179 [07:08<07:17,  4.86s/it]\u001b[A\n",
            "Iteration:  50% 90/179 [07:13<07:10,  4.84s/it]\u001b[A\n",
            "Iteration:  51% 91/179 [07:18<07:04,  4.83s/it]\u001b[A\n",
            "Iteration:  51% 92/179 [07:23<06:59,  4.82s/it]\u001b[A\n",
            "Iteration:  52% 93/179 [07:28<06:54,  4.82s/it]\u001b[A\n",
            "Iteration:  53% 94/179 [07:32<06:48,  4.81s/it]\u001b[A\n",
            "Iteration:  53% 95/179 [07:37<06:43,  4.80s/it]\u001b[A\n",
            "Iteration:  54% 96/179 [07:42<06:38,  4.80s/it]\u001b[A\n",
            "Iteration:  54% 97/179 [07:47<06:34,  4.81s/it]\u001b[A\n",
            "Iteration:  55% 98/179 [07:52<06:29,  4.81s/it]\u001b[A\n",
            "Iteration:  55% 99/179 [07:56<06:24,  4.81s/it]\u001b[A\n",
            "Iteration:  56% 100/179 [08:01<06:19,  4.81s/it]\u001b[A\n",
            "Iteration:  56% 101/179 [08:06<06:15,  4.81s/it]\u001b[A\n",
            "Iteration:  57% 102/179 [08:11<06:10,  4.81s/it]\u001b[A\n",
            "Iteration:  58% 103/179 [08:16<06:05,  4.81s/it]\u001b[A\n",
            "Iteration:  58% 104/179 [08:20<06:00,  4.81s/it]\u001b[A\n",
            "Iteration:  59% 105/179 [08:25<05:55,  4.81s/it]\u001b[A\n",
            "Iteration:  59% 106/179 [08:30<05:50,  4.80s/it]\u001b[A\n",
            "Iteration:  60% 107/179 [08:35<05:45,  4.80s/it]\u001b[A\n",
            "Iteration:  60% 108/179 [08:40<05:41,  4.81s/it]\u001b[A\n",
            "Iteration:  61% 109/179 [08:44<05:36,  4.81s/it]\u001b[A\n",
            "Iteration:  61% 110/179 [08:49<05:32,  4.81s/it]\u001b[A\n",
            "Iteration:  62% 111/179 [08:54<05:27,  4.81s/it]\u001b[A\n",
            "Iteration:  63% 112/179 [08:59<05:22,  4.81s/it]\u001b[A\n",
            "Iteration:  63% 113/179 [09:04<05:17,  4.81s/it]\u001b[A\n",
            "Iteration:  64% 114/179 [09:08<05:12,  4.81s/it]\u001b[A\n",
            "Iteration:  64% 115/179 [09:13<05:08,  4.82s/it]\u001b[A\n",
            "Iteration:  65% 116/179 [09:18<05:03,  4.82s/it]\u001b[A\n",
            "Iteration:  65% 117/179 [09:23<04:58,  4.82s/it]\u001b[A\n",
            "Iteration:  66% 118/179 [09:28<04:53,  4.82s/it]\u001b[A\n",
            "Iteration:  66% 119/179 [09:33<04:49,  4.82s/it]\u001b[A\n",
            "Iteration:  67% 120/179 [09:37<04:44,  4.83s/it]\u001b[A\n",
            "Iteration:  68% 121/179 [09:42<04:39,  4.82s/it]\u001b[A\n",
            "Iteration:  68% 122/179 [09:47<04:35,  4.83s/it]\u001b[A\n",
            "Iteration:  69% 123/179 [09:52<04:30,  4.83s/it]\u001b[A\n",
            "Iteration:  69% 124/179 [09:57<04:25,  4.83s/it]\u001b[A\n",
            "Iteration:  70% 125/179 [10:02<04:20,  4.82s/it]\u001b[A\n",
            "Iteration:  70% 126/179 [10:06<04:15,  4.82s/it]\u001b[A\n",
            "Iteration:  71% 127/179 [10:11<04:10,  4.83s/it]\u001b[A\n",
            "Iteration:  72% 128/179 [10:16<04:05,  4.82s/it]\u001b[A\n",
            "Iteration:  72% 129/179 [10:21<04:01,  4.82s/it]\u001b[A\n",
            "Iteration:  73% 130/179 [10:26<03:56,  4.82s/it]\u001b[A\n",
            "Iteration:  73% 131/179 [10:31<03:51,  4.83s/it]\u001b[A\n",
            "Iteration:  74% 132/179 [10:35<03:46,  4.82s/it]\u001b[A\n",
            "Iteration:  74% 133/179 [10:40<03:41,  4.82s/it]\u001b[A\n",
            "Iteration:  75% 134/179 [10:45<03:37,  4.83s/it]\u001b[A\n",
            "Iteration:  75% 135/179 [10:50<03:32,  4.83s/it]\u001b[A\n",
            "Iteration:  76% 136/179 [10:55<03:27,  4.82s/it]\u001b[A\n",
            "Iteration:  77% 137/179 [10:59<03:22,  4.82s/it]\u001b[A\n",
            "Iteration:  77% 138/179 [11:04<03:17,  4.82s/it]\u001b[A\n",
            "Iteration:  78% 139/179 [11:09<03:12,  4.82s/it]\u001b[A\n",
            "Iteration:  78% 140/179 [11:14<03:08,  4.82s/it]\u001b[A\n",
            "Iteration:  79% 141/179 [11:19<03:03,  4.82s/it]\u001b[A\n",
            "Iteration:  79% 142/179 [11:24<02:58,  4.82s/it]\u001b[A\n",
            "Iteration:  80% 143/179 [11:28<02:53,  4.82s/it]\u001b[A\n",
            "Iteration:  80% 144/179 [11:33<02:48,  4.82s/it]\u001b[A\n",
            "Iteration:  81% 145/179 [11:38<02:43,  4.82s/it]\u001b[A\n",
            "Iteration:  82% 146/179 [11:43<02:39,  4.82s/it]\u001b[A\n",
            "Iteration:  82% 147/179 [11:48<02:34,  4.81s/it]\u001b[A\n",
            "Iteration:  83% 148/179 [11:52<02:29,  4.81s/it]\u001b[A\n",
            "Iteration:  83% 149/179 [11:57<02:24,  4.82s/it]\u001b[A\n",
            "Iteration:  84% 150/179 [12:02<02:19,  4.82s/it]\u001b[A\n",
            "Iteration:  84% 151/179 [12:07<02:15,  4.83s/it]\u001b[A\n",
            "Iteration:  85% 152/179 [12:12<02:10,  4.82s/it]\u001b[A\n",
            "Iteration:  85% 153/179 [12:17<02:05,  4.81s/it]\u001b[A\n",
            "Iteration:  86% 154/179 [12:21<02:00,  4.81s/it]\u001b[A\n",
            "Iteration:  87% 155/179 [12:26<01:55,  4.81s/it]\u001b[A\n",
            "Iteration:  87% 156/179 [12:31<01:50,  4.80s/it]\u001b[A\n",
            "Iteration:  88% 157/179 [12:36<01:45,  4.80s/it]\u001b[A\n",
            "Iteration:  88% 158/179 [12:41<01:41,  4.81s/it]\u001b[A\n",
            "Iteration:  89% 159/179 [12:45<01:36,  4.81s/it]\u001b[A\n",
            "Iteration:  89% 160/179 [12:50<01:31,  4.81s/it]\u001b[A\n",
            "Iteration:  90% 161/179 [12:55<01:26,  4.82s/it]\u001b[A\n",
            "Iteration:  91% 162/179 [13:00<01:21,  4.82s/it]\u001b[A\n",
            "Iteration:  91% 163/179 [13:05<01:17,  4.82s/it]\u001b[A\n",
            "Iteration:  92% 164/179 [13:09<01:12,  4.81s/it]\u001b[A\n",
            "Iteration:  92% 165/179 [13:14<01:07,  4.82s/it]\u001b[A\n",
            "Iteration:  93% 166/179 [13:19<01:02,  4.83s/it]\u001b[A\n",
            "Iteration:  93% 167/179 [13:24<00:57,  4.82s/it]\u001b[A\n",
            "Iteration:  94% 168/179 [13:29<00:53,  4.83s/it]\u001b[A\n",
            "Iteration:  94% 169/179 [13:34<00:48,  4.83s/it]\u001b[A\n",
            "Iteration:  95% 170/179 [13:38<00:43,  4.83s/it]\u001b[A\n",
            "Iteration:  96% 171/179 [13:43<00:38,  4.83s/it]\u001b[A\n",
            "Iteration:  96% 172/179 [13:48<00:33,  4.84s/it]\u001b[A\n",
            "Iteration:  97% 173/179 [13:53<00:28,  4.83s/it]\u001b[A\n",
            "Iteration:  97% 174/179 [13:58<00:24,  4.83s/it]\u001b[A\n",
            "Iteration:  98% 175/179 [14:03<00:19,  4.83s/it]\u001b[A\n",
            "Iteration:  98% 176/179 [14:07<00:14,  4.84s/it]\u001b[A\n",
            "Iteration:  99% 177/179 [14:12<00:09,  4.83s/it]\u001b[A\n",
            "Iteration:  99% 178/179 [14:17<00:04,  4.83s/it]\u001b[A\n",
            "Iteration: 100% 179/179 [14:21<00:00,  4.81s/it]\n",
            "Epoch:  20% 1/5 [14:21<57:25, 861.38s/it]\n",
            "Iteration:   0% 0/179 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/179 [00:04<14:23,  4.85s/it]\u001b[A\n",
            "Iteration:   1% 2/179 [00:09<14:18,  4.85s/it]\u001b[A\n",
            "Iteration:   2% 3/179 [00:14<14:12,  4.84s/it]\u001b[A\n",
            "Iteration:   2% 4/179 [00:19<14:06,  4.84s/it]\u001b[A\n",
            "Iteration:   3% 5/179 [00:24<13:59,  4.83s/it]\u001b[A\n",
            "Iteration:   3% 6/179 [00:28<13:55,  4.83s/it]\u001b[A\n",
            "Iteration:   4% 7/179 [00:33<13:50,  4.83s/it]\u001b[A\n",
            "Iteration:   4% 8/179 [00:38<13:45,  4.83s/it]\u001b[A\n",
            "Iteration:   5% 9/179 [00:43<13:41,  4.83s/it]\u001b[A\n",
            "Iteration:   6% 10/179 [00:48<13:37,  4.84s/it]\u001b[A\n",
            "Iteration:   6% 11/179 [00:53<13:33,  4.84s/it]\u001b[A\n",
            "Iteration:   7% 12/179 [00:57<13:27,  4.84s/it]\u001b[A\n",
            "Iteration:   7% 13/179 [01:02<13:22,  4.83s/it]\u001b[A\n",
            "Iteration:   8% 14/179 [01:07<13:17,  4.83s/it]\u001b[A\n",
            "Iteration:   8% 15/179 [01:12<13:13,  4.84s/it]\u001b[A\n",
            "Iteration:   9% 16/179 [01:17<13:07,  4.83s/it]\u001b[A\n",
            "Iteration:   9% 17/179 [01:22<13:03,  4.84s/it]\u001b[A\n",
            "Iteration:  10% 18/179 [01:27<12:58,  4.84s/it]\u001b[A\n",
            "Iteration:  11% 19/179 [01:31<12:54,  4.84s/it]\u001b[A\n",
            "Iteration:  11% 20/179 [01:36<12:49,  4.84s/it]\u001b[A\n",
            "Iteration:  12% 21/179 [01:41<12:43,  4.83s/it]\u001b[A\n",
            "Iteration:  12% 22/179 [01:46<12:38,  4.83s/it]\u001b[A\n",
            "Iteration:  13% 23/179 [01:51<12:33,  4.83s/it]\u001b[A\n",
            "Iteration:  13% 24/179 [01:56<12:29,  4.83s/it]\u001b[A\n",
            "Iteration:  14% 25/179 [02:00<12:24,  4.83s/it]\u001b[A\n",
            "Iteration:  15% 26/179 [02:05<12:19,  4.83s/it]\u001b[A\n",
            "Iteration:  15% 27/179 [02:10<12:16,  4.84s/it]\u001b[A\n",
            "Iteration:  16% 28/179 [02:15<12:10,  4.84s/it]\u001b[A\n",
            "Iteration:  16% 29/179 [02:20<12:05,  4.84s/it]\u001b[A\n",
            "Iteration:  17% 30/179 [02:25<12:01,  4.84s/it]\u001b[A\n",
            "Iteration:  17% 31/179 [02:29<11:56,  4.84s/it]\u001b[A\n",
            "Iteration:  18% 32/179 [02:34<11:50,  4.83s/it]\u001b[A\n",
            "Iteration:  18% 33/179 [02:39<11:45,  4.83s/it]\u001b[A\n",
            "Iteration:  19% 34/179 [02:44<11:41,  4.84s/it]\u001b[A\n",
            "Iteration:  20% 35/179 [02:49<11:36,  4.84s/it]\u001b[A\n",
            "Iteration:  20% 36/179 [02:54<11:31,  4.84s/it]\u001b[A\n",
            "Iteration:  21% 37/179 [02:58<11:26,  4.84s/it]\u001b[A\n",
            "Iteration:  21% 38/179 [03:03<11:21,  4.84s/it]\u001b[A\n",
            "Iteration:  22% 39/179 [03:08<11:17,  4.84s/it]\u001b[A\n",
            "Iteration:  22% 40/179 [03:13<11:12,  4.84s/it]\u001b[A\n",
            "Iteration:  23% 41/179 [03:18<11:08,  4.84s/it]\u001b[A\n",
            "Iteration:  23% 42/179 [03:23<11:02,  4.84s/it]\u001b[A\n",
            "Iteration:  24% 43/179 [03:27<10:57,  4.83s/it]\u001b[A\n",
            "Iteration:  25% 44/179 [03:32<10:52,  4.83s/it]\u001b[A\n",
            "Iteration:  25% 45/179 [03:37<10:48,  4.84s/it]\u001b[A\n",
            "Iteration:  26% 46/179 [03:42<10:44,  4.85s/it]\u001b[A\n",
            "Iteration:  26% 47/179 [03:47<10:38,  4.84s/it]\u001b[A\n",
            "Iteration:  27% 48/179 [03:52<10:33,  4.84s/it]\u001b[A\n",
            "Iteration:  27% 49/179 [03:56<10:28,  4.83s/it]\u001b[A\n",
            "Iteration:  28% 50/179 [04:01<10:23,  4.83s/it]\u001b[A\n",
            "Iteration:  28% 51/179 [04:06<10:18,  4.83s/it]\u001b[A\n",
            "Iteration:  29% 52/179 [04:11<10:13,  4.83s/it]\u001b[A\n",
            "Iteration:  30% 53/179 [04:16<10:08,  4.83s/it]\u001b[A\n",
            "Iteration:  30% 54/179 [04:21<10:04,  4.83s/it]\u001b[A\n",
            "Iteration:  31% 55/179 [04:25<09:58,  4.83s/it]\u001b[A\n",
            "Iteration:  31% 56/179 [04:30<09:54,  4.83s/it]\u001b[A\n",
            "Iteration:  32% 57/179 [04:35<09:48,  4.83s/it]\u001b[A\n",
            "Iteration:  32% 58/179 [04:40<09:44,  4.83s/it]\u001b[A\n",
            "Iteration:  33% 59/179 [04:45<09:39,  4.83s/it]\u001b[A\n",
            "Iteration:  34% 60/179 [04:50<09:35,  4.83s/it]\u001b[A\n",
            "Iteration:  34% 61/179 [04:54<09:30,  4.83s/it]\u001b[A\n",
            "Iteration:  35% 62/179 [04:59<09:25,  4.83s/it]\u001b[A\n",
            "Iteration:  35% 63/179 [05:04<09:20,  4.83s/it]\u001b[A\n",
            "Iteration:  36% 64/179 [05:09<09:14,  4.82s/it]\u001b[A\n",
            "Iteration:  36% 65/179 [05:14<09:09,  4.82s/it]\u001b[A\n",
            "Iteration:  37% 66/179 [05:18<09:04,  4.82s/it]\u001b[A\n",
            "Iteration:  37% 67/179 [05:23<09:00,  4.82s/it]\u001b[A\n",
            "Iteration:  38% 68/179 [05:28<08:55,  4.83s/it]\u001b[A\n",
            "Iteration:  39% 69/179 [05:33<08:51,  4.83s/it]\u001b[A\n",
            "Iteration:  39% 70/179 [05:38<08:46,  4.83s/it]\u001b[A\n",
            "Iteration:  40% 71/179 [05:43<08:41,  4.82s/it]\u001b[A\n",
            "Iteration:  40% 72/179 [05:47<08:35,  4.82s/it]\u001b[A\n",
            "Iteration:  41% 73/179 [05:52<08:31,  4.83s/it]\u001b[A\n",
            "Iteration:  41% 74/179 [05:57<08:27,  4.83s/it]\u001b[A\n",
            "Iteration:  42% 75/179 [06:02<08:22,  4.83s/it]\u001b[A\n",
            "Iteration:  42% 76/179 [06:07<08:17,  4.83s/it]\u001b[A\n",
            "Iteration:  43% 77/179 [06:12<08:12,  4.83s/it]\u001b[A\n",
            "Iteration:  44% 78/179 [06:16<08:08,  4.84s/it]\u001b[A\n",
            "Iteration:  44% 79/179 [06:21<08:03,  4.83s/it]\u001b[A\n",
            "Iteration:  45% 80/179 [06:26<07:57,  4.83s/it]\u001b[A\n",
            "Iteration:  45% 81/179 [06:31<07:52,  4.82s/it]\u001b[A\n",
            "Iteration:  46% 82/179 [06:36<07:48,  4.83s/it]\u001b[A\n",
            "Iteration:  46% 83/179 [06:41<07:43,  4.83s/it]\u001b[A\n",
            "Iteration:  47% 84/179 [06:45<07:38,  4.83s/it]\u001b[A\n",
            "Iteration:  47% 85/179 [06:50<07:34,  4.84s/it]\u001b[A\n",
            "Iteration:  48% 86/179 [06:55<07:30,  4.84s/it]\u001b[A\n",
            "Iteration:  49% 87/179 [07:00<07:25,  4.84s/it]\u001b[A\n",
            "Iteration:  49% 88/179 [07:05<07:19,  4.84s/it]\u001b[A\n",
            "Iteration:  50% 89/179 [07:10<07:14,  4.83s/it]\u001b[A\n",
            "Iteration:  50% 90/179 [07:14<07:09,  4.83s/it]\u001b[A\n",
            "Iteration:  51% 91/179 [07:19<07:05,  4.84s/it]\u001b[A\n",
            "Iteration:  51% 92/179 [07:24<07:01,  4.84s/it]\u001b[A\n",
            "Iteration:  52% 93/179 [07:29<06:56,  4.84s/it]\u001b[A\n",
            "Iteration:  53% 94/179 [07:34<06:52,  4.85s/it]\u001b[A\n",
            "Iteration:  53% 95/179 [07:39<06:47,  4.85s/it]\u001b[A\n",
            "Iteration:  54% 96/179 [07:44<06:41,  4.84s/it]\u001b[A\n",
            "Iteration:  54% 97/179 [07:48<06:37,  4.85s/it]\u001b[A\n",
            "Iteration:  55% 98/179 [07:53<06:32,  4.84s/it]\u001b[A\n",
            "Iteration:  55% 99/179 [07:58<06:27,  4.84s/it]\u001b[A\n",
            "Iteration:  56% 100/179 [08:03<06:22,  4.84s/it]\u001b[A\n",
            "Iteration:  56% 101/179 [08:08<06:17,  4.84s/it]\u001b[A\n",
            "Iteration:  57% 102/179 [08:13<06:12,  4.84s/it]\u001b[A\n",
            "Iteration:  58% 103/179 [08:17<06:07,  4.83s/it]\u001b[A\n",
            "Iteration:  58% 104/179 [08:22<06:02,  4.83s/it]\u001b[A\n",
            "Iteration:  59% 105/179 [08:27<05:57,  4.84s/it]\u001b[A\n",
            "Iteration:  59% 106/179 [08:32<05:53,  4.84s/it]\u001b[A\n",
            "Iteration:  60% 107/179 [08:37<05:48,  4.84s/it]\u001b[A\n",
            "Iteration:  60% 108/179 [08:42<05:42,  4.83s/it]\u001b[A\n",
            "Iteration:  61% 109/179 [08:46<05:38,  4.83s/it]\u001b[A\n",
            "Iteration:  61% 110/179 [08:51<05:33,  4.83s/it]\u001b[A\n",
            "Iteration:  62% 111/179 [08:56<05:28,  4.84s/it]\u001b[A\n",
            "Iteration:  63% 112/179 [09:01<05:24,  4.84s/it]\u001b[A\n",
            "Iteration:  63% 113/179 [09:06<05:19,  4.84s/it]\u001b[A\n",
            "Iteration:  64% 114/179 [09:11<05:14,  4.84s/it]\u001b[A\n",
            "Iteration:  64% 115/179 [09:15<05:09,  4.83s/it]\u001b[A\n",
            "Iteration:  65% 116/179 [09:20<05:04,  4.84s/it]\u001b[A\n",
            "Iteration:  65% 117/179 [09:25<04:59,  4.83s/it]\u001b[A\n",
            "Iteration:  66% 118/179 [09:30<04:54,  4.83s/it]\u001b[A\n",
            "Iteration:  66% 119/179 [09:35<04:49,  4.83s/it]\u001b[A\n",
            "Iteration:  67% 120/179 [09:40<04:44,  4.83s/it]\u001b[A\n",
            "Iteration:  68% 121/179 [09:44<04:39,  4.82s/it]\u001b[A\n",
            "Iteration:  68% 122/179 [09:49<04:34,  4.82s/it]\u001b[A\n",
            "Iteration:  69% 123/179 [09:54<04:30,  4.83s/it]\u001b[A\n",
            "Iteration:  69% 124/179 [09:59<04:25,  4.82s/it]\u001b[A\n",
            "Iteration:  70% 125/179 [10:04<04:20,  4.82s/it]\u001b[A\n",
            "Iteration:  70% 126/179 [10:08<04:15,  4.83s/it]\u001b[A\n",
            "Iteration:  71% 127/179 [10:13<04:11,  4.83s/it]\u001b[A\n",
            "Iteration:  72% 128/179 [10:18<04:06,  4.82s/it]\u001b[A\n",
            "Iteration:  72% 129/179 [10:23<04:01,  4.82s/it]\u001b[A\n",
            "Iteration:  73% 130/179 [10:28<03:56,  4.82s/it]\u001b[A\n",
            "Iteration:  73% 131/179 [10:33<03:51,  4.82s/it]\u001b[A\n",
            "Iteration:  74% 132/179 [10:37<03:46,  4.82s/it]\u001b[A\n",
            "Iteration:  74% 133/179 [10:42<03:41,  4.82s/it]\u001b[A\n",
            "Iteration:  75% 134/179 [10:47<03:37,  4.83s/it]\u001b[A\n",
            "Iteration:  75% 135/179 [10:52<03:32,  4.82s/it]\u001b[A\n",
            "Iteration:  76% 136/179 [10:57<03:27,  4.83s/it]\u001b[A\n",
            "Iteration:  77% 137/179 [11:02<03:22,  4.82s/it]\u001b[A\n",
            "Iteration:  77% 138/179 [11:06<03:17,  4.83s/it]\u001b[A\n",
            "Iteration:  78% 139/179 [11:11<03:13,  4.83s/it]\u001b[A\n",
            "Iteration:  78% 140/179 [11:16<03:08,  4.83s/it]\u001b[A\n",
            "Iteration:  79% 141/179 [11:21<03:03,  4.82s/it]\u001b[A\n",
            "Iteration:  79% 142/179 [11:26<02:58,  4.82s/it]\u001b[A\n",
            "Iteration:  80% 143/179 [11:30<02:53,  4.83s/it]\u001b[A\n",
            "Iteration:  80% 144/179 [11:35<02:49,  4.83s/it]\u001b[A\n",
            "Iteration:  81% 145/179 [11:40<02:44,  4.83s/it]\u001b[A\n",
            "Iteration:  82% 146/179 [11:45<02:39,  4.83s/it]\u001b[A\n",
            "Iteration:  82% 147/179 [11:50<02:34,  4.83s/it]\u001b[A\n",
            "Iteration:  83% 148/179 [11:55<02:29,  4.83s/it]\u001b[A\n",
            "Iteration:  83% 149/179 [11:59<02:24,  4.83s/it]\u001b[A\n",
            "Iteration:  84% 150/179 [12:04<02:19,  4.82s/it]\u001b[A\n",
            "Iteration:  84% 151/179 [12:09<02:15,  4.82s/it]\u001b[A\n",
            "Iteration:  85% 152/179 [12:14<02:10,  4.82s/it]\u001b[A\n",
            "Iteration:  85% 153/179 [12:19<02:05,  4.82s/it]\u001b[A\n",
            "Iteration:  86% 154/179 [12:24<02:00,  4.83s/it]\u001b[A\n",
            "Iteration:  87% 155/179 [12:28<01:55,  4.83s/it]\u001b[A\n",
            "Iteration:  87% 156/179 [12:33<01:51,  4.83s/it]\u001b[A\n",
            "Iteration:  88% 157/179 [12:38<01:46,  4.83s/it]\u001b[A\n",
            "Iteration:  88% 158/179 [12:43<01:41,  4.83s/it]\u001b[A\n",
            "Iteration:  89% 159/179 [12:48<01:36,  4.82s/it]\u001b[A\n",
            "Iteration:  89% 160/179 [12:53<01:31,  4.83s/it]\u001b[A\n",
            "Iteration:  90% 161/179 [12:57<01:26,  4.82s/it]\u001b[A\n",
            "Iteration:  91% 162/179 [13:02<01:22,  4.82s/it]\u001b[A\n",
            "Iteration:  91% 163/179 [13:07<01:17,  4.83s/it]\u001b[A\n",
            "Iteration:  92% 164/179 [13:12<01:12,  4.83s/it]\u001b[A\n",
            "Iteration:  92% 165/179 [13:17<01:07,  4.82s/it]\u001b[A\n",
            "Iteration:  93% 166/179 [13:21<01:02,  4.82s/it]\u001b[A\n",
            "Iteration:  93% 167/179 [13:26<00:57,  4.82s/it]\u001b[A\n",
            "Iteration:  94% 168/179 [13:31<00:53,  4.82s/it]\u001b[A\n",
            "Iteration:  94% 169/179 [13:36<00:48,  4.82s/it]\u001b[A\n",
            "Iteration:  95% 170/179 [13:41<00:43,  4.82s/it]\u001b[A\n",
            "Iteration:  96% 171/179 [13:46<00:38,  4.82s/it]\u001b[A\n",
            "Iteration:  96% 172/179 [13:50<00:33,  4.82s/it]\u001b[A\n",
            "Iteration:  97% 173/179 [13:55<00:28,  4.82s/it]\u001b[A\n",
            "Iteration:  97% 174/179 [14:00<00:24,  4.83s/it]\u001b[A\n",
            "Iteration:  98% 175/179 [14:05<00:19,  4.83s/it]\u001b[A\n",
            "Iteration:  98% 176/179 [14:10<00:14,  4.83s/it]\u001b[A\n",
            "Iteration:  99% 177/179 [14:15<00:09,  4.83s/it]\u001b[A\n",
            "Iteration:  99% 178/179 [14:19<00:04,  4.83s/it]\u001b[A\n",
            "Iteration: 100% 179/179 [14:23<00:00,  4.83s/it]\n",
            "Epoch:  40% 2/5 [28:45<43:06, 862.07s/it]\n",
            "Iteration:   0% 0/179 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/179 [00:04<14:20,  4.83s/it]\u001b[A\n",
            "Iteration:   1% 2/179 [00:09<14:15,  4.83s/it]\u001b[A\n",
            "Iteration:   2% 3/179 [00:14<14:10,  4.83s/it]\u001b[A\n",
            "Iteration:   2% 4/179 [00:19<14:04,  4.83s/it]\u001b[A\n",
            "Iteration:   3% 5/179 [00:24<14:00,  4.83s/it]\u001b[A\n",
            "Iteration:   3% 6/179 [00:28<13:55,  4.83s/it]\u001b[A\n",
            "Iteration:   4% 7/179 [00:33<13:50,  4.83s/it]\u001b[A\n",
            "Iteration:   4% 8/179 [00:38<13:45,  4.83s/it]\u001b[A\n",
            "Iteration:   5% 9/179 [00:43<13:40,  4.82s/it]\u001b[A\n",
            "Iteration:   6% 10/179 [00:48<13:36,  4.83s/it]\u001b[A\n",
            "Iteration:   6% 11/179 [00:53<13:30,  4.83s/it]\u001b[A\n",
            "Iteration:   7% 12/179 [00:57<13:25,  4.82s/it]\u001b[A\n",
            "Iteration:   7% 13/179 [01:02<13:21,  4.83s/it]\u001b[A\n",
            "Iteration:   8% 14/179 [01:07<13:15,  4.82s/it]\u001b[A\n",
            "Iteration:   8% 15/179 [01:12<13:11,  4.83s/it]\u001b[A\n",
            "Iteration:   9% 16/179 [01:17<13:05,  4.82s/it]\u001b[A\n",
            "Iteration:   9% 17/179 [01:22<13:01,  4.83s/it]\u001b[A\n",
            "Iteration:  10% 18/179 [01:26<12:57,  4.83s/it]\u001b[A\n",
            "Iteration:  11% 19/179 [01:31<12:52,  4.83s/it]\u001b[A\n",
            "Iteration:  11% 20/179 [01:36<12:47,  4.83s/it]\u001b[A\n",
            "Iteration:  12% 21/179 [01:41<12:41,  4.82s/it]\u001b[A\n",
            "Iteration:  12% 22/179 [01:46<12:36,  4.82s/it]\u001b[A\n",
            "Iteration:  13% 23/179 [01:50<12:32,  4.82s/it]\u001b[A\n",
            "Iteration:  13% 24/179 [01:55<12:28,  4.83s/it]\u001b[A\n",
            "Iteration:  14% 25/179 [02:00<12:23,  4.83s/it]\u001b[A\n",
            "Iteration:  15% 26/179 [02:05<12:17,  4.82s/it]\u001b[A\n",
            "Iteration:  15% 27/179 [02:10<12:13,  4.83s/it]\u001b[A\n",
            "Iteration:  16% 28/179 [02:15<12:08,  4.82s/it]\u001b[A\n",
            "Iteration:  16% 29/179 [02:19<12:03,  4.82s/it]\u001b[A\n",
            "Iteration:  17% 30/179 [02:24<11:59,  4.83s/it]\u001b[A\n",
            "Iteration:  17% 31/179 [02:29<11:54,  4.83s/it]\u001b[A\n",
            "Iteration:  18% 32/179 [02:34<11:49,  4.83s/it]\u001b[A\n",
            "Iteration:  18% 33/179 [02:39<11:45,  4.83s/it]\u001b[A\n",
            "Iteration:  19% 34/179 [02:44<11:40,  4.83s/it]\u001b[A\n",
            "Iteration:  20% 35/179 [02:48<11:37,  4.84s/it]\u001b[A\n",
            "Iteration:  20% 36/179 [02:53<11:30,  4.83s/it]\u001b[A\n",
            "Iteration:  21% 37/179 [02:58<11:25,  4.83s/it]\u001b[A\n",
            "Iteration:  21% 38/179 [03:03<11:20,  4.83s/it]\u001b[A\n",
            "Iteration:  22% 39/179 [03:08<11:15,  4.83s/it]\u001b[A\n",
            "Iteration:  22% 40/179 [03:13<11:10,  4.83s/it]\u001b[A\n",
            "Iteration:  23% 41/179 [03:17<11:05,  4.82s/it]\u001b[A\n",
            "Iteration:  23% 42/179 [03:22<11:01,  4.83s/it]\u001b[A\n",
            "Iteration:  24% 43/179 [03:27<10:56,  4.83s/it]\u001b[A\n",
            "Iteration:  25% 44/179 [03:32<10:51,  4.83s/it]\u001b[A\n",
            "Iteration:  25% 45/179 [03:37<10:47,  4.83s/it]\u001b[A\n",
            "Iteration:  26% 46/179 [03:42<10:41,  4.82s/it]\u001b[A\n",
            "Iteration:  26% 47/179 [03:46<10:36,  4.82s/it]\u001b[A\n",
            "Iteration:  27% 48/179 [03:51<10:31,  4.82s/it]\u001b[A\n",
            "Iteration:  27% 49/179 [03:56<10:26,  4.82s/it]\u001b[A\n",
            "Iteration:  28% 50/179 [04:01<10:21,  4.82s/it]\u001b[A\n",
            "Iteration:  28% 51/179 [04:06<10:17,  4.82s/it]\u001b[A\n",
            "Iteration:  29% 52/179 [04:10<10:12,  4.82s/it]\u001b[A\n",
            "Iteration:  30% 53/179 [04:15<10:07,  4.82s/it]\u001b[A\n",
            "Iteration:  30% 54/179 [04:20<10:02,  4.82s/it]\u001b[A\n",
            "Iteration:  31% 55/179 [04:25<09:57,  4.82s/it]\u001b[A\n",
            "Iteration:  31% 56/179 [04:30<09:52,  4.82s/it]\u001b[A\n",
            "Iteration:  32% 57/179 [04:35<09:48,  4.83s/it]\u001b[A\n",
            "Iteration:  32% 58/179 [04:39<09:45,  4.84s/it]\u001b[A\n",
            "Iteration:  33% 59/179 [04:44<09:39,  4.83s/it]\u001b[A\n",
            "Iteration:  34% 60/179 [04:49<09:35,  4.83s/it]\u001b[A\n",
            "Iteration:  34% 61/179 [04:54<09:31,  4.84s/it]\u001b[A\n",
            "Iteration:  35% 62/179 [04:59<09:26,  4.84s/it]\u001b[A\n",
            "Iteration:  35% 63/179 [05:04<09:21,  4.84s/it]\u001b[A\n",
            "Iteration:  36% 64/179 [05:08<09:16,  4.84s/it]\u001b[A\n",
            "Iteration:  36% 65/179 [05:13<09:10,  4.83s/it]\u001b[A\n",
            "Iteration:  37% 66/179 [05:18<09:06,  4.83s/it]\u001b[A\n",
            "Iteration:  37% 67/179 [05:23<09:00,  4.82s/it]\u001b[A\n",
            "Iteration:  38% 68/179 [05:28<08:54,  4.82s/it]\u001b[A\n",
            "Iteration:  39% 69/179 [05:33<08:49,  4.81s/it]\u001b[A\n",
            "Iteration:  39% 70/179 [05:37<08:45,  4.82s/it]\u001b[A\n",
            "Iteration:  40% 71/179 [05:42<08:39,  4.81s/it]\u001b[A\n",
            "Iteration:  40% 72/179 [05:47<08:35,  4.81s/it]\u001b[A\n",
            "Iteration:  41% 73/179 [05:52<08:30,  4.82s/it]\u001b[A\n",
            "Iteration:  41% 74/179 [05:57<08:25,  4.82s/it]\u001b[A\n",
            "Iteration:  42% 75/179 [06:01<08:20,  4.81s/it]\u001b[A\n",
            "Iteration:  42% 76/179 [06:06<08:16,  4.82s/it]\u001b[A\n",
            "Iteration:  43% 77/179 [06:11<08:11,  4.82s/it]\u001b[A\n",
            "Iteration:  44% 78/179 [06:16<08:07,  4.83s/it]\u001b[A\n",
            "Iteration:  44% 79/179 [06:21<08:02,  4.83s/it]\u001b[A\n",
            "Iteration:  45% 80/179 [06:26<07:58,  4.83s/it]\u001b[A\n",
            "Iteration:  45% 81/179 [06:30<07:52,  4.83s/it]\u001b[A\n",
            "Iteration:  46% 82/179 [06:35<07:47,  4.82s/it]\u001b[A\n",
            "Iteration:  46% 83/179 [06:40<07:42,  4.82s/it]\u001b[A\n",
            "Iteration:  47% 84/179 [06:45<07:38,  4.82s/it]\u001b[A\n",
            "Iteration:  47% 85/179 [06:50<07:33,  4.83s/it]\u001b[A\n",
            "Iteration:  48% 86/179 [06:55<07:28,  4.83s/it]\u001b[A\n",
            "Iteration:  49% 87/179 [06:59<07:23,  4.83s/it]\u001b[A\n",
            "Iteration:  49% 88/179 [07:04<07:19,  4.83s/it]\u001b[A\n",
            "Iteration:  50% 89/179 [07:09<07:14,  4.83s/it]\u001b[A\n",
            "Iteration:  50% 90/179 [07:14<07:09,  4.83s/it]\u001b[A\n",
            "Iteration:  51% 91/179 [07:19<07:05,  4.83s/it]\u001b[A\n",
            "Iteration:  51% 92/179 [07:23<07:00,  4.83s/it]\u001b[A\n",
            "Iteration:  52% 93/179 [07:28<06:55,  4.83s/it]\u001b[A\n",
            "Iteration:  53% 94/179 [07:33<06:50,  4.83s/it]\u001b[A\n",
            "Iteration:  53% 95/179 [07:38<06:45,  4.83s/it]\u001b[A\n",
            "Iteration:  54% 96/179 [07:43<06:40,  4.83s/it]\u001b[A\n",
            "Iteration:  54% 97/179 [07:48<06:36,  4.84s/it]\u001b[A\n",
            "Iteration:  55% 98/179 [07:52<06:31,  4.83s/it]\u001b[A\n",
            "Iteration:  55% 99/179 [07:57<06:26,  4.83s/it]\u001b[A\n",
            "Iteration:  56% 100/179 [08:02<06:21,  4.83s/it]\u001b[A\n",
            "Iteration:  56% 101/179 [08:07<06:16,  4.83s/it]\u001b[A\n",
            "Iteration:  57% 102/179 [08:12<06:11,  4.83s/it]\u001b[A\n",
            "Iteration:  58% 103/179 [08:17<06:07,  4.83s/it]\u001b[A\n",
            "Iteration:  58% 104/179 [08:21<06:02,  4.83s/it]\u001b[A\n",
            "Iteration:  59% 105/179 [08:26<05:57,  4.83s/it]\u001b[A\n",
            "Iteration:  59% 106/179 [08:31<05:53,  4.84s/it]\u001b[A\n",
            "Iteration:  60% 107/179 [08:36<05:48,  4.83s/it]\u001b[A\n",
            "Iteration:  60% 108/179 [08:41<05:43,  4.84s/it]\u001b[A\n",
            "Iteration:  61% 109/179 [08:46<05:38,  4.84s/it]\u001b[A\n",
            "Iteration:  61% 110/179 [08:50<05:33,  4.83s/it]\u001b[A\n",
            "Iteration:  62% 111/179 [08:55<05:28,  4.82s/it]\u001b[A\n",
            "Iteration:  63% 112/179 [09:00<05:23,  4.82s/it]\u001b[A\n",
            "Iteration:  63% 113/179 [09:05<05:18,  4.82s/it]\u001b[A\n",
            "Iteration:  64% 114/179 [09:10<05:13,  4.82s/it]\u001b[A\n",
            "Iteration:  64% 115/179 [09:15<05:08,  4.82s/it]\u001b[A\n",
            "Iteration:  65% 116/179 [09:19<05:03,  4.82s/it]\u001b[A\n",
            "Iteration:  65% 117/179 [09:24<04:59,  4.82s/it]\u001b[A\n",
            "Iteration:  66% 118/179 [09:29<04:53,  4.82s/it]\u001b[A\n",
            "Iteration:  66% 119/179 [09:34<04:48,  4.81s/it]\u001b[A\n",
            "Iteration:  67% 120/179 [09:39<04:44,  4.82s/it]\u001b[A\n",
            "Iteration:  68% 121/179 [09:43<04:39,  4.82s/it]\u001b[A\n",
            "Iteration:  68% 122/179 [09:48<04:34,  4.82s/it]\u001b[A\n",
            "Iteration:  69% 123/179 [09:53<04:29,  4.82s/it]\u001b[A\n",
            "Iteration:  69% 124/179 [09:58<04:24,  4.82s/it]\u001b[A\n",
            "Iteration:  70% 125/179 [10:03<04:20,  4.82s/it]\u001b[A\n",
            "Iteration:  70% 126/179 [10:08<04:15,  4.82s/it]\u001b[A\n",
            "Iteration:  71% 127/179 [10:12<04:10,  4.82s/it]\u001b[A\n",
            "Iteration:  72% 128/179 [10:17<04:06,  4.83s/it]\u001b[A\n",
            "Iteration:  72% 129/179 [10:22<04:00,  4.82s/it]\u001b[A\n",
            "Iteration:  73% 130/179 [10:27<03:56,  4.82s/it]\u001b[A\n",
            "Iteration:  73% 131/179 [10:32<03:51,  4.82s/it]\u001b[A\n",
            "Iteration:  74% 132/179 [10:36<03:46,  4.82s/it]\u001b[A\n",
            "Iteration:  74% 133/179 [10:41<03:41,  4.82s/it]\u001b[A\n",
            "Iteration:  75% 134/179 [10:46<03:36,  4.82s/it]\u001b[A\n",
            "Iteration:  75% 135/179 [10:51<03:32,  4.83s/it]\u001b[A\n",
            "Iteration:  76% 136/179 [10:56<03:27,  4.83s/it]\u001b[A\n",
            "Iteration:  77% 137/179 [11:01<03:22,  4.83s/it]\u001b[A\n",
            "Iteration:  77% 138/179 [11:05<03:17,  4.83s/it]\u001b[A\n",
            "Iteration:  78% 139/179 [11:10<03:12,  4.82s/it]\u001b[A\n",
            "Iteration:  78% 140/179 [11:15<03:08,  4.82s/it]\u001b[A\n",
            "Iteration:  79% 141/179 [11:20<03:03,  4.82s/it]\u001b[A\n",
            "Iteration:  79% 142/179 [11:25<02:58,  4.82s/it]\u001b[A\n",
            "Iteration:  80% 143/179 [11:30<02:53,  4.83s/it]\u001b[A\n",
            "Iteration:  80% 144/179 [11:34<02:48,  4.83s/it]\u001b[A\n",
            "Iteration:  81% 145/179 [11:39<02:43,  4.82s/it]\u001b[A\n",
            "Iteration:  82% 146/179 [11:44<02:39,  4.82s/it]\u001b[A\n",
            "Iteration:  82% 147/179 [11:49<02:34,  4.82s/it]\u001b[A\n",
            "Iteration:  83% 148/179 [11:54<02:29,  4.83s/it]\u001b[A\n",
            "Iteration:  83% 149/179 [11:58<02:24,  4.83s/it]\u001b[A\n",
            "Iteration:  84% 150/179 [12:03<02:20,  4.83s/it]\u001b[A\n",
            "Iteration:  84% 151/179 [12:08<02:15,  4.82s/it]\u001b[A\n",
            "Iteration:  85% 152/179 [12:13<02:10,  4.83s/it]\u001b[A\n",
            "Iteration:  85% 153/179 [12:18<02:05,  4.82s/it]\u001b[A\n",
            "Iteration:  86% 154/179 [12:23<02:00,  4.83s/it]\u001b[A\n",
            "Iteration:  87% 155/179 [12:27<01:55,  4.83s/it]\u001b[A\n",
            "Iteration:  87% 156/179 [12:32<01:50,  4.82s/it]\u001b[A\n",
            "Iteration:  88% 157/179 [12:37<01:46,  4.82s/it]\u001b[A\n",
            "Iteration:  88% 158/179 [12:42<01:41,  4.83s/it]\u001b[A\n",
            "Iteration:  89% 159/179 [12:47<01:36,  4.83s/it]\u001b[A\n",
            "Iteration:  89% 160/179 [12:52<01:31,  4.83s/it]\u001b[A\n",
            "Iteration:  90% 161/179 [12:56<01:27,  4.83s/it]\u001b[A\n",
            "Iteration:  91% 162/179 [13:01<01:22,  4.84s/it]\u001b[A\n",
            "Iteration:  91% 163/179 [13:06<01:17,  4.83s/it]\u001b[A\n",
            "Iteration:  92% 164/179 [13:11<01:12,  4.83s/it]\u001b[A\n",
            "Iteration:  92% 165/179 [13:16<01:07,  4.83s/it]\u001b[A\n",
            "Iteration:  93% 166/179 [13:21<01:02,  4.83s/it]\u001b[A\n",
            "Iteration:  93% 167/179 [13:25<00:57,  4.82s/it]\u001b[A\n",
            "Iteration:  94% 168/179 [13:30<00:53,  4.82s/it]\u001b[A\n",
            "Iteration:  94% 169/179 [13:35<00:48,  4.82s/it]\u001b[A\n",
            "Iteration:  95% 170/179 [13:40<00:43,  4.82s/it]\u001b[A\n",
            "Iteration:  96% 171/179 [13:45<00:38,  4.82s/it]\u001b[A\n",
            "Iteration:  96% 172/179 [13:49<00:33,  4.82s/it]\u001b[A\n",
            "Iteration:  97% 173/179 [13:54<00:28,  4.83s/it]\u001b[A\n",
            "Iteration:  97% 174/179 [13:59<00:24,  4.84s/it]\u001b[A\n",
            "Iteration:  98% 175/179 [14:04<00:19,  4.84s/it]\u001b[A\n",
            "Iteration:  98% 176/179 [14:09<00:14,  4.82s/it]\u001b[A\n",
            "Iteration:  99% 177/179 [14:14<00:09,  4.82s/it]\u001b[A\n",
            "Iteration:  99% 178/179 [14:18<00:04,  4.82s/it]\u001b[A\n",
            "Iteration: 100% 179/179 [14:22<00:00,  4.82s/it]\n",
            "Epoch:  60% 3/5 [43:07<28:44, 862.27s/it]\n",
            "Iteration:   0% 0/179 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/179 [00:04<14:17,  4.82s/it]\u001b[A\n",
            "Iteration:   1% 2/179 [00:09<14:12,  4.82s/it]\u001b[A\n",
            "Iteration:   2% 3/179 [00:14<14:08,  4.82s/it]\u001b[A\n",
            "Iteration:   2% 4/179 [00:19<14:03,  4.82s/it]\u001b[A\n",
            "Iteration:   3% 5/179 [00:24<13:59,  4.82s/it]\u001b[A\n",
            "Iteration:   3% 6/179 [00:28<13:57,  4.84s/it]\u001b[A\n",
            "Iteration:   4% 7/179 [00:33<13:53,  4.85s/it]\u001b[A\n",
            "Iteration:   4% 8/179 [00:38<13:49,  4.85s/it]\u001b[A\n",
            "Iteration:   5% 9/179 [00:43<13:43,  4.84s/it]\u001b[A\n",
            "Iteration:   6% 10/179 [00:48<13:37,  4.84s/it]\u001b[A\n",
            "Iteration:   6% 11/179 [00:53<13:32,  4.84s/it]\u001b[A\n",
            "Iteration:   7% 12/179 [00:58<13:26,  4.83s/it]\u001b[A\n",
            "Iteration:   7% 13/179 [01:02<13:21,  4.83s/it]\u001b[A\n",
            "Iteration:   8% 14/179 [01:07<13:17,  4.83s/it]\u001b[A\n",
            "Iteration:   8% 15/179 [01:12<13:13,  4.84s/it]\u001b[A\n",
            "Iteration:   9% 16/179 [01:17<13:07,  4.83s/it]\u001b[A\n",
            "Iteration:   9% 17/179 [01:22<13:02,  4.83s/it]\u001b[A\n",
            "Iteration:  10% 18/179 [01:27<12:57,  4.83s/it]\u001b[A\n",
            "Iteration:  11% 19/179 [01:31<12:52,  4.83s/it]\u001b[A\n",
            "Iteration:  11% 20/179 [01:36<12:47,  4.83s/it]\u001b[A\n",
            "Iteration:  12% 21/179 [01:41<12:42,  4.83s/it]\u001b[A\n",
            "Iteration:  12% 22/179 [01:46<12:38,  4.83s/it]\u001b[A\n",
            "Iteration:  13% 23/179 [01:51<12:33,  4.83s/it]\u001b[A\n",
            "Iteration:  13% 24/179 [01:55<12:28,  4.83s/it]\u001b[A\n",
            "Iteration:  14% 25/179 [02:00<12:24,  4.84s/it]\u001b[A\n",
            "Iteration:  15% 26/179 [02:05<12:20,  4.84s/it]\u001b[A\n",
            "Iteration:  15% 27/179 [02:10<12:16,  4.84s/it]\u001b[A\n",
            "Iteration:  16% 28/179 [02:15<12:11,  4.84s/it]\u001b[A\n",
            "Iteration:  16% 29/179 [02:20<12:07,  4.85s/it]\u001b[A\n",
            "Iteration:  17% 30/179 [02:25<12:01,  4.84s/it]\u001b[A\n",
            "Iteration:  17% 31/179 [02:29<11:57,  4.85s/it]\u001b[A\n",
            "Iteration:  18% 32/179 [02:34<11:52,  4.84s/it]\u001b[A\n",
            "Iteration:  18% 33/179 [02:39<11:47,  4.85s/it]\u001b[A\n",
            "Iteration:  19% 34/179 [02:44<11:42,  4.85s/it]\u001b[A\n",
            "Iteration:  20% 35/179 [02:49<11:37,  4.85s/it]\u001b[A\n",
            "Iteration:  20% 36/179 [02:54<11:33,  4.85s/it]\u001b[A\n",
            "Iteration:  21% 37/179 [02:58<11:27,  4.84s/it]\u001b[A\n",
            "Iteration:  21% 38/179 [03:03<11:22,  4.84s/it]\u001b[A\n",
            "Iteration:  22% 39/179 [03:08<11:17,  4.84s/it]\u001b[A\n",
            "Iteration:  22% 40/179 [03:13<11:12,  4.84s/it]\u001b[A\n",
            "Iteration:  23% 41/179 [03:18<11:07,  4.84s/it]\u001b[A\n",
            "Iteration:  23% 42/179 [03:23<11:02,  4.84s/it]\u001b[A\n",
            "Iteration:  24% 43/179 [03:28<10:58,  4.84s/it]\u001b[A\n",
            "Iteration:  25% 44/179 [03:32<10:53,  4.84s/it]\u001b[A\n",
            "Iteration:  25% 45/179 [03:37<10:48,  4.84s/it]\u001b[A\n",
            "Iteration:  26% 46/179 [03:42<10:43,  4.84s/it]\u001b[A\n",
            "Iteration:  26% 47/179 [03:47<10:37,  4.83s/it]\u001b[A\n",
            "Iteration:  27% 48/179 [03:52<10:32,  4.83s/it]\u001b[A\n",
            "Iteration:  27% 49/179 [03:57<10:28,  4.83s/it]\u001b[A\n",
            "Iteration:  28% 50/179 [04:01<10:23,  4.83s/it]\u001b[A\n",
            "Iteration:  28% 51/179 [04:06<10:19,  4.84s/it]\u001b[A\n",
            "Iteration:  29% 52/179 [04:11<10:14,  4.84s/it]\u001b[A\n",
            "Iteration:  30% 53/179 [04:16<10:09,  4.84s/it]\u001b[A\n",
            "Iteration:  30% 54/179 [04:21<10:04,  4.84s/it]\u001b[A\n",
            "Iteration:  31% 55/179 [04:26<09:59,  4.83s/it]\u001b[A\n",
            "Iteration:  31% 56/179 [04:30<09:53,  4.83s/it]\u001b[A\n",
            "Iteration:  32% 57/179 [04:35<09:49,  4.84s/it]\u001b[A\n",
            "Iteration:  32% 58/179 [04:40<09:46,  4.85s/it]\u001b[A\n",
            "Iteration:  33% 59/179 [04:45<09:44,  4.87s/it]\u001b[A\n",
            "Iteration:  34% 60/179 [04:50<09:38,  4.86s/it]\u001b[A\n",
            "Iteration:  34% 61/179 [04:55<09:31,  4.84s/it]\u001b[A\n",
            "Iteration:  35% 62/179 [04:59<09:26,  4.84s/it]\u001b[A\n",
            "Iteration:  35% 63/179 [05:04<09:20,  4.83s/it]\u001b[A\n",
            "Iteration:  36% 64/179 [05:09<09:15,  4.83s/it]\u001b[A\n",
            "Iteration:  36% 65/179 [05:14<09:10,  4.83s/it]\u001b[A\n",
            "Iteration:  37% 66/179 [05:19<09:05,  4.83s/it]\u001b[A\n",
            "Iteration:  37% 67/179 [05:24<09:00,  4.83s/it]\u001b[A\n",
            "Iteration:  38% 68/179 [05:28<08:55,  4.83s/it]\u001b[A\n",
            "Iteration:  39% 69/179 [05:33<08:50,  4.82s/it]\u001b[A\n",
            "Iteration:  39% 70/179 [05:38<08:46,  4.83s/it]\u001b[A\n",
            "Iteration:  40% 71/179 [05:43<08:41,  4.83s/it]\u001b[A\n",
            "Iteration:  40% 72/179 [05:48<08:36,  4.83s/it]\u001b[A\n",
            "Iteration:  41% 73/179 [05:53<08:31,  4.82s/it]\u001b[A\n",
            "Iteration:  41% 74/179 [05:57<08:26,  4.82s/it]\u001b[A\n",
            "Iteration:  42% 75/179 [06:02<08:21,  4.82s/it]\u001b[A\n",
            "Iteration:  42% 76/179 [06:07<08:16,  4.82s/it]\u001b[A\n",
            "Iteration:  43% 77/179 [06:12<08:12,  4.83s/it]\u001b[A\n",
            "Iteration:  44% 78/179 [06:17<08:06,  4.82s/it]\u001b[A\n",
            "Iteration:  44% 79/179 [06:21<08:02,  4.82s/it]\u001b[A\n",
            "Iteration:  45% 80/179 [06:26<07:57,  4.83s/it]\u001b[A\n",
            "Iteration:  45% 81/179 [06:31<07:52,  4.82s/it]\u001b[A\n",
            "Iteration:  46% 82/179 [06:36<07:47,  4.82s/it]\u001b[A\n",
            "Iteration:  46% 83/179 [06:41<07:42,  4.82s/it]\u001b[A\n",
            "Iteration:  47% 84/179 [06:46<07:37,  4.82s/it]\u001b[A\n",
            "Iteration:  47% 85/179 [06:50<07:33,  4.82s/it]\u001b[A\n",
            "Iteration:  48% 86/179 [06:55<07:27,  4.82s/it]\u001b[A\n",
            "Iteration:  49% 87/179 [07:00<07:23,  4.82s/it]\u001b[A\n",
            "Iteration:  49% 88/179 [07:05<07:18,  4.82s/it]\u001b[A\n",
            "Iteration:  50% 89/179 [07:10<07:14,  4.82s/it]\u001b[A\n",
            "Iteration:  50% 90/179 [07:15<07:09,  4.83s/it]\u001b[A\n",
            "Iteration:  51% 91/179 [07:19<07:05,  4.84s/it]\u001b[A\n",
            "Iteration:  51% 92/179 [07:24<07:01,  4.84s/it]\u001b[A\n",
            "Iteration:  52% 93/179 [07:29<06:55,  4.83s/it]\u001b[A\n",
            "Iteration:  53% 94/179 [07:34<06:50,  4.83s/it]\u001b[A\n",
            "Iteration:  53% 95/179 [07:39<06:46,  4.83s/it]\u001b[A\n",
            "Iteration:  54% 96/179 [07:44<06:41,  4.84s/it]\u001b[A\n",
            "Iteration:  54% 97/179 [07:48<06:36,  4.84s/it]\u001b[A\n",
            "Iteration:  55% 98/179 [07:53<06:31,  4.83s/it]\u001b[A\n",
            "Iteration:  55% 99/179 [07:58<06:26,  4.84s/it]\u001b[A\n",
            "Iteration:  56% 100/179 [08:03<06:22,  4.84s/it]\u001b[A\n",
            "Iteration:  56% 101/179 [08:08<06:17,  4.84s/it]\u001b[A\n",
            "Iteration:  57% 102/179 [08:13<06:12,  4.84s/it]\u001b[A\n",
            "Iteration:  58% 103/179 [08:17<06:07,  4.84s/it]\u001b[A\n",
            "Iteration:  58% 104/179 [08:22<06:02,  4.84s/it]\u001b[A\n",
            "Iteration:  59% 105/179 [08:27<05:57,  4.84s/it]\u001b[A\n",
            "Iteration:  59% 106/179 [08:32<05:52,  4.83s/it]\u001b[A\n",
            "Iteration:  60% 107/179 [08:37<05:48,  4.84s/it]\u001b[A\n",
            "Iteration:  60% 108/179 [08:42<05:43,  4.84s/it]\u001b[A\n",
            "Iteration:  61% 109/179 [08:46<05:37,  4.83s/it]\u001b[A\n",
            "Iteration:  61% 110/179 [08:51<05:33,  4.83s/it]\u001b[A\n",
            "Iteration:  62% 111/179 [08:56<05:28,  4.82s/it]\u001b[A\n",
            "Iteration:  63% 112/179 [09:01<05:23,  4.83s/it]\u001b[A\n",
            "Iteration:  63% 113/179 [09:06<05:19,  4.83s/it]\u001b[A\n",
            "Iteration:  64% 114/179 [09:11<05:14,  4.84s/it]\u001b[A\n",
            "Iteration:  64% 115/179 [09:15<05:09,  4.84s/it]\u001b[A\n",
            "Iteration:  65% 116/179 [09:20<05:04,  4.83s/it]\u001b[A\n",
            "Iteration:  65% 117/179 [09:25<04:58,  4.82s/it]\u001b[A\n",
            "Iteration:  66% 118/179 [09:30<04:54,  4.83s/it]\u001b[A\n",
            "Iteration:  66% 119/179 [09:35<04:49,  4.83s/it]\u001b[A\n",
            "Iteration:  67% 120/179 [09:40<04:44,  4.83s/it]\u001b[A\n",
            "Iteration:  68% 121/179 [09:44<04:40,  4.83s/it]\u001b[A\n",
            "Iteration:  68% 122/179 [09:49<04:35,  4.84s/it]\u001b[A\n",
            "Iteration:  69% 123/179 [09:54<04:31,  4.84s/it]\u001b[A\n",
            "Iteration:  69% 124/179 [09:59<04:26,  4.84s/it]\u001b[A\n",
            "Iteration:  70% 125/179 [10:04<04:21,  4.83s/it]\u001b[A\n",
            "Iteration:  70% 126/179 [10:09<04:16,  4.84s/it]\u001b[A\n",
            "Iteration:  71% 127/179 [10:13<04:11,  4.84s/it]\u001b[A\n",
            "Iteration:  72% 128/179 [10:18<04:06,  4.84s/it]\u001b[A\n",
            "Iteration:  72% 129/179 [10:23<04:02,  4.85s/it]\u001b[A\n",
            "Iteration:  73% 130/179 [10:28<03:57,  4.85s/it]\u001b[A\n",
            "Iteration:  73% 131/179 [10:33<03:53,  4.85s/it]\u001b[A\n",
            "Iteration:  74% 132/179 [10:38<03:48,  4.85s/it]\u001b[A\n",
            "Iteration:  74% 133/179 [10:43<03:43,  4.85s/it]\u001b[A\n",
            "Iteration:  75% 134/179 [10:47<03:38,  4.85s/it]\u001b[A\n",
            "Iteration:  75% 135/179 [10:52<03:33,  4.84s/it]\u001b[A\n",
            "Iteration:  76% 136/179 [10:57<03:28,  4.84s/it]\u001b[A\n",
            "Iteration:  77% 137/179 [11:02<03:22,  4.83s/it]\u001b[A\n",
            "Iteration:  77% 138/179 [11:07<03:18,  4.83s/it]\u001b[A\n",
            "Iteration:  78% 139/179 [11:12<03:13,  4.84s/it]\u001b[A\n",
            "Iteration:  78% 140/179 [11:16<03:08,  4.83s/it]\u001b[A"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5F5-S4gr1Ha"
      },
      "source": [
        "!rm -rf out_ner"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKfJy70K1dnA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}